import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import IPython.display as ipd  # To play sound in the notebook
import os
from tqdm import tqdm
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from pydub import AudioSegment, effects
import noisereduce as nr


RAVDESS = "Data/audio_speech_actors_01-24/"
EMOTIONS = {
    1: 'neutral',
    2: 'calm',
    3: 'happy',
    4: 'sad',
    5: 'angry',
    6: 'fear',
    7: 'disgust',
    8: 'surprise'
}
SAMPLE_RATE = 48000


folder_names = os.listdir(RAVDESS)
folder_names.sort()

file_emotions = []
file_gender = []
file_intensity = []
file_paths = []

for i in folder_names:
    if os.path.isdir(RAVDESS + i):
        file_names = os.listdir(RAVDESS + i)

        for file in file_names:
            parts = file.split('.')[0].split('-')
            file_emotions.append(int(parts[2]))
            file_intensity.append(int(parts[3]))
            if int(parts[6]) % 2 == 0:
                file_gender.append('female')
            else:
                file_gender.append('male')
            file_paths.append(RAVDESS + i + '/' + file)
# print(file_emotions, file_gender, file_intensity, file_paths, sep="\n")


dataset = pd.DataFrame({
    'path': file_paths,
    'intensity': file_intensity,
    'gender': file_gender,
    'emotion': file_emotions
})
dataset['emotion'] = dataset['emotion'].map(EMOTIONS)
print('Shape=>', dataset.shape)
dataset.head(50)


dataset['gender'].value_counts()


dataset['intensity'].value_counts()


dataset['emotion'].value_counts()


dataset['emotion'].value_counts(normalize=True) * 100


male_sample = []
female_sample = []
for i in EMOTIONS.values():
    male_sample.append(dataset[dataset['emotion'] == i][
        dataset['gender'] == 'male']['path'].reset_index(drop=True)[0])
    female_sample.append(dataset[dataset['emotion'] == i][
        dataset['gender'] == 'female']['path'].reset_index(drop=True)[0])
print(male_sample)
print(female_sample)


for emotion, path in zip(EMOTIONS.values(), male_sample):
    signal, sr = librosa.load(path, sr=SAMPLE_RATE)
    print(f"Emotion:{emotion}")
    # Play the Audio
    ipd.display(ipd.Audio(signal, rate=SAMPLE_RATE))


for emotion, path in zip(EMOTIONS.values(), female_sample):
    signal, sr = librosa.load(path, sr=SAMPLE_RATE)
    print(f"Emotion:{emotion}")
    # Play the Audio
    ipd.display(ipd.Audio(signal, rate=SAMPLE_RATE))


fig, axes = plt.subplots(4, 2, figsize=(24, 22))
plt.suptitle("\n\n\nMale Samples", va='center', fontweight="bold")
for ax, emotion, path in zip(axes.flatten(), EMOTIONS.values(), male_sample):
    signal, sr = librosa.load(path, sr=SAMPLE_RATE)
    librosa.display.waveshow(signal, sr=SAMPLE_RATE, ax=ax)
    ax.set(title=emotion)


fig, axes = plt.subplots(4, 2, figsize=(24, 22))
plt.suptitle("\n\n\nFemale Samples", va='center', fontweight="bold")
for ax, emotion, path in zip(axes.flatten(), EMOTIONS.values(), female_sample):
    signal, sr = librosa.load(path, sr=SAMPLE_RATE)
    librosa.display.waveshow(signal, sr=SAMPLE_RATE, ax=ax)
    ax.set(title=emotion)


#Taking an audio file form male_sample to perform preprocessing to know the audio data more
path = male_sample[0]
rawsound = AudioSegment.from_file(path)
ats, sr = librosa.load(path, sr=None)
print("Sample Rate : ", sr)
plt.figure(figsize=(12, 2))
librosa.display.waveshow(ats, sr)
plt.title('Initial audio')

rawsound


X = librosa.stft(ats)
Xdb = librosa.amplitude_to_db(abs(X))
plt.figure(figsize=(18, 8))
librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')
plt.colorbar()


plt.figure(figsize=(18, 8))
librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')
plt.colorbar()


# Normalizing the audio to give audio boost to quiter part by giving +5.0 dBFS (decibels relative to full scale)
# and transforing normalized audio to Numpy array
normalizedsound = effects.normalize(rawsound, headroom=5.0)
normal_audio = np.array(normalizedsound.get_array_of_samples(),
                        dtype='float32')

plt.figure(figsize=(12, 2))
librosa.display.waveshow(normal_audio, sr)
plt.title('Normalized audio')

normalizedsound


# Trimming the audio to remover silence before and after speech
trimmed_audio, index = librosa.effects.trim(normal_audio, top_db=30)

plt.figure(figsize=(12, 2))
librosa.display.waveshow(trimmed_audio, sr)
plt.title('Trimmed audio')

ipd.display(ipd.Audio(data=trimmed_audio, rate=sr))


# Padding some values to right side to make all data length equal
# so we have maximum lenght audio file with 173056

padded_audio = np.pad(trimmed_audio, (0, 173056 - len(trimmed_audio)),
                      'constant')

plt.figure(figsize=(12, 2))
librosa.display.waveshow(padded_audio, sr)
plt.title('Padded audio')

ipd.display(ipd.Audio(data=padded_audio, rate=sr))


# Eliminating noise from audio by performing Noise Reduction
final_audio = nr.reduce_noise(padded_audio, sr)

plt.figure(figsize=(12, 2))
librosa.display.waveshow(final_audio, sr)
plt.title('Reduced Noise audio')

ipd.display(ipd.Audio(data=final_audio, rate=sr))



