{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f263c8c-799d-4b11-8b42-70e93164a0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import pyaudio\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from keras.models import load_model\n",
    "import noisereduce as nr\n",
    "import math, os, sys, time\n",
    "import random\n",
    "from threading import Thread\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "plt.style.use('dark_background')\n",
    "plt.rc('figure', titlesize=16)\n",
    "plt.rc('axes', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a3ac86-6f6a-4ad3-8eb4-2e0956ae0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class emotion_detector:\n",
    "    # ['angry', 'calm', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "    def __init__(self,\n",
    "                 model_path,\n",
    "                 gender_classifier=None,\n",
    "                 sample_rate=22050,\n",
    "                 threshold=122):\n",
    "        self.MODEL = load_model(model_path)\n",
    "        if gender_classifier != None:\n",
    "            self.GC_MODEL = load_model(gender_classifier)\n",
    "        else:\n",
    "            self.GC_MODEL = False\n",
    "        EMOTIONS = {\n",
    "            0: 'Angry',\n",
    "            1: 'Calm',\n",
    "            2: 'Disgust',\n",
    "            3: 'Fearful',\n",
    "            4: 'Happy',\n",
    "            5: 'Sad',\n",
    "            6: 'Surprised'\n",
    "        }\n",
    "        self.genders = {0: 'Female', 1: 'Male'}\n",
    "        self.ENC = OneHotEncoder()\n",
    "        self.ENC.fit_transform([['Angry'], ['Calm'], ['Disgust'], ['Fearful'],\n",
    "                                ['Happy'], ['Sad'], ['Surprised']])\n",
    "        self.THRESHOLD = threshold\n",
    "        self.FORMAT = pyaudio.paFloat32\n",
    "        self.CHANNELS = 1\n",
    "        self.RATE = sample_rate\n",
    "        self.sr = self.RATE\n",
    "        self.CHUNK = 1024 * 4\n",
    "        self.COLORS = [\n",
    "            'red', 'lightblue', 'peru', 'darkred', 'green', 'yellow',\n",
    "            'lightpink'\n",
    "        ]\n",
    "        self.EMOTION_LIST = list(EMOTIONS.values())\n",
    "        self.emotion = \"Silence\"\n",
    "        self.gender = \"\"\n",
    "        self.predictions = [0, 0, 0, 0, 0, 0, 0]\n",
    "        self.total_predictions = []\n",
    "        self.LENGTH = 121212\n",
    "        self.AUDIO = pyaudio.PyAudio()\n",
    "        self.FRAME_LENGTH = 2048\n",
    "        self.HOP_LENGTH = 512\n",
    "        self.RECORD_SECONDS = 2.6\n",
    "        self.audio = (np.sin(np.pi * np.arange(self.RECORD_SECONDS) * 54 /\n",
    "                             self.sr)).astype(np.float32)\n",
    "\n",
    "        self.SCALER = StandardScaler()\n",
    "        self.FIG, self.AXES = plt.subplots(1,\n",
    "                                           2,\n",
    "                                           figsize=(14, 8),\n",
    "                                           tight_layout=True)\n",
    "        self.FIG.canvas.mpl_connect('close_event', self.stop_stream)\n",
    "        self.stop_flag = False\n",
    "        self.emo_color = {\n",
    "            self.EMOTION_LIST[i]: self.COLORS[i]\n",
    "            for i in range(len(self.EMOTION_LIST))\n",
    "        }\n",
    "        self.emo_color[\"Silence\"] = 'white'\n",
    "\n",
    "    def __analyser(self, frame):\n",
    "        self.AXES[0].clear()\n",
    "        self.AXES[1].clear()\n",
    "\n",
    "        self.AXES[0].set_ylim(0, 1.2)\n",
    "        self.AXES[1].set_ylim(-1, 1)\n",
    "\n",
    "        plt.suptitle(\"\\n\\n\\n\" + self.emotion.capitalize() + \" \" + self.gender,\n",
    "                     va='center',\n",
    "                     fontweight=\"bold\")\n",
    "        self.AXES[0].bar(self.EMOTION_LIST, self.predictions, color=self.COLORS)\n",
    "\n",
    "        librosa.display.waveshow(y=self.audio,\n",
    "                                 sr=self.sr,\n",
    "                                 ax=self.AXES[1],\n",
    "                                 color=self.emo_color[self.emotion])\n",
    "\n",
    "    def __extract_features(self, audio, sr):\n",
    "        rms = []\n",
    "        mfcc = []\n",
    "        mel = []\n",
    "\n",
    "        # Fetch the sample rate.\n",
    "        normalizedsound = librosa.util.normalize(audio)\n",
    "\n",
    "        # Trim silence from the beginning and the end.\n",
    "        trimmed_audio, index = librosa.effects.trim(y=normalizedsound,\n",
    "                                                    top_db=30)\n",
    "\n",
    "        final_audio = np.pad(trimmed_audio,\n",
    "                             (0, self.LENGTH - len(trimmed_audio)), 'constant')\n",
    "        # Noise reduction.\n",
    "        final_audio = nr.reduce_noise(y=final_audio,\n",
    "                                      sr=self.RATE)  #updated 03/03/22\n",
    "\n",
    "        f1 = librosa.feature.rms(\n",
    "            y=final_audio,\n",
    "            frame_length=self.FRAME_LENGTH,\n",
    "            hop_length=self.HOP_LENGTH).T  # Energy - Root Mean Square\n",
    "\n",
    "        f2 = librosa.feature.melspectrogram(y=final_audio,\n",
    "                                            sr=sr,\n",
    "                                            n_fft=self.FRAME_LENGTH,\n",
    "                                            hop_length=self.HOP_LENGTH).T\n",
    "\n",
    "        f3 = librosa.feature.mfcc(y=final_audio,\n",
    "                                  sr=sr,\n",
    "                                  n_mfcc=40,\n",
    "                                  hop_length=self.HOP_LENGTH).T  # MFCC\n",
    "\n",
    "        # Filling the data lists\n",
    "\n",
    "        rms.append(self.SCALER.fit_transform(f1))\n",
    "        mel.append(self.SCALER.fit_transform(f2))\n",
    "        mfcc.append(self.SCALER.fit_transform(f3))\n",
    "\n",
    "        f_rms = np.asarray(rms)\n",
    "        f_mel = np.asarray(mel)\n",
    "        f_mfccs = np.asarray(mfcc)\n",
    "\n",
    "        # Concatenating all features to 'X' variable.\n",
    "        features = np.concatenate((f_rms, f_mel, f_mfccs), axis=2)\n",
    "        return features\n",
    "\n",
    "    def __emotion(self, audio_features):\n",
    "        predictions = self.MODEL.predict(audio_features,\n",
    "                                         use_multiprocessing=True)\n",
    "        # print(predictions)\n",
    "\n",
    "        max_emo = self.ENC.inverse_transform(predictions)\n",
    "        pred_list = list(predictions)\n",
    "        predictions = np.squeeze(np.array(pred_list).tolist(), axis=0)\n",
    "        # print(predictions)\n",
    "\n",
    "        return predictions, max_emo[0][0]\n",
    "\n",
    "    def __gender(self, audio_features):\n",
    "        if not self.GC_MODEL:\n",
    "            return \"\"\n",
    "        else:\n",
    "            predictions = self.GC_MODEL.predict(audio_features,\n",
    "                                                use_multiprocessing=True)\n",
    "            print(predictions)\n",
    "\n",
    "            prediction = int(predictions.round()[0][0])\n",
    "            # predictions = np.squeeze(np.array(pred_list).tolist(), axis=0)\n",
    "            print(prediction)\n",
    "\n",
    "            return self.genders[prediction]\n",
    "\n",
    "    def list_devices(self):\n",
    "        print(\"----------------------record device list---------------------\")\n",
    "        info = self.AUDIO.get_host_api_info_by_index(0)\n",
    "        numdevices = info.get('deviceCount')\n",
    "        for i in range(0, numdevices):\n",
    "            if (self.AUDIO.get_device_info_by_host_api_device_index(\n",
    "                    0, i).get('maxInputChannels')) > 0:\n",
    "                print(\n",
    "                    \"Input Device id \", i, \" - \",\n",
    "                    self.AUDIO.get_device_info_by_host_api_device_index(\n",
    "                        0, i).get('name'))\n",
    "\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "\n",
    "        index = int(input())\n",
    "        return index\n",
    "\n",
    "    def silence(self, audio):\n",
    "        threshold = (sum(audio) / len(audio))\n",
    "        # print(threshold)\n",
    "        return (sum(audio) / len(audio)) < self.THRESHOLD\n",
    "\n",
    "    def __start(self, file=None, device_index=None):\n",
    "        self.total_predictions = []\n",
    "\n",
    "        if file != None:\n",
    "            try:\n",
    "                audio_data, self.sr = librosa.load(file)\n",
    "                ipd.display(ipd.Audio(data=audio_data, rate=self.sr))\n",
    "                n = len(audio_data) / self.LENGTH\n",
    "                if n < 1:\n",
    "                    self.audio = audio_data\n",
    "                    features = self.__extract_features(audio_data, self.sr)\n",
    "                    self.predictions, self.emotion = self.__emotion(features)\n",
    "                    self.gender = self.__gender(features)\n",
    "                    print(self.predictions, self.emotion)\n",
    "\n",
    "                else:\n",
    "                    for i in range(math.floor(n)):\n",
    "                        self.audio = audio_data[self.LENGTH * i:self.LENGTH *\n",
    "                                                (i + 1)]\n",
    "                        features = self.__extract_features(audio, self.sr)\n",
    "                        self.predictions, self.emotion = self.__emotion(\n",
    "                            features)\n",
    "                        self.gender = self.__gender(features)\n",
    "                        self.total_predictions.append(self.predictions)\n",
    "                        print(self.predictions, self.emotion)\n",
    "\n",
    "                    else:\n",
    "                        self.audio = audio_data[self.LENGTH * i:]\n",
    "                        features = self.__extract_features(audio, self.sr)\n",
    "                        self.predictions, self.emotion = self.__emotion(\n",
    "                            features)\n",
    "                        self.gender = self.__gender(features)\n",
    "                        self.total_predictions.append(self.predictions)\n",
    "                        print(self.predictions, self.emotion)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        else:\n",
    "            if device_index == None:\n",
    "                print(\"Missing Device Index Or File !\")\n",
    "                sys.exit(1)\n",
    "                # index = self.list_devices()\n",
    "            print(\"recording via index \" + str(device_index))\n",
    "\n",
    "            self.STREAM = self.AUDIO.open(format=self.FORMAT,\n",
    "                                          channels=self.CHANNELS,\n",
    "                                          rate=self.RATE,\n",
    "                                          input=True,\n",
    "                                          input_device_index=device_index,\n",
    "                                          frames_per_buffer=self.CHUNK)\n",
    "            self.sr = self.RATE\n",
    "            n = int(self.RATE / self.CHUNK * self.RECORD_SECONDS)\n",
    "            # print(n)\n",
    "            # try:\n",
    "            while not self.stop_flag:\n",
    "                # print(\"recording started\")\n",
    "                Recordframes = []\n",
    "                for i in range(0, n):\n",
    "                    data = self.STREAM.read(self.CHUNK,\n",
    "                                            exception_on_overflow=False)\n",
    "                    Recordframes.append(data)\n",
    "                # print (\"recording stopped\")\n",
    "                # print(len(Recordframes))\n",
    "                self.audio = np.frombuffer(b''.join(Recordframes),\n",
    "                                           dtype=np.float32)\n",
    "                # ipd.display(ipd.Audio(data=self.audio, rate=self.RATE))\n",
    "                # time.sleep(5)\n",
    "                if self.silence(b''.join(Recordframes[-4:])):\n",
    "                    # print(\"Silence Detected !\")\n",
    "                    self.emotion = \"Silence\"\n",
    "                    self.gender = \"\"\n",
    "                    self.predictions = [0, 0, 0, 0, 0, 0, 0]\n",
    "                else:\n",
    "                    features = self.__extract_features(self.audio, self.RATE)\n",
    "                    self.gender = self.__gender(features)\n",
    "                    self.predictions, self.emotion = self.__emotion(features)\n",
    "                    self.total_predictions.append(self.predictions)\n",
    "                    # print(emotion)\n",
    "\n",
    "        print(\"Main Thread Terminated !\")\n",
    "\n",
    "    def start_stream(self, file=None, device_index=None):\n",
    "        print(\"Stream Started !\")\n",
    "        self.main_thread = Thread(target=self.__start,\n",
    "                                  args=(file, device_index))\n",
    "        self.main_thread.start()\n",
    "        self.anim = FuncAnimation(fig=self.FIG,\n",
    "                                  func=self.__analyser,\n",
    "                                  interval=1)\n",
    "        plt.show()\n",
    "        FIG, AXES = plt.subplots(1, 1, figsize=(14, 8), tight_layout=True)\n",
    "        AXES.set_ylim(0, 1.2)\n",
    "        plt.suptitle(\"\\n\\n\\nSummary\", va='center', fontweight=\"bold\")\n",
    "        total_predictions = np.mean(np.array(self.total_predictions).tolist(),\n",
    "                                    axis=0)\n",
    "        print(total_predictions)\n",
    "        AXES.bar(self.EMOTION_LIST, total_predictions, color=self.COLORS)\n",
    "        plt.show()\n",
    "\n",
    "    def stop_stream(self, event=None):\n",
    "        self.stop_flag = True\n",
    "        self.main_thread.join()\n",
    "        self.STREAM.stop_stream()\n",
    "        self.STREAM.close()\n",
    "        self.AUDIO.terminate()\n",
    "        print(\"Stream Stoped !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6cf6574-1d1c-4843-975b-d58047b91341",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------record device list---------------------\n",
      "Input Device id  0  -  MacBook Pro Microphone\n",
      "Input Device id  2  -  Kishan’s AirPods Pro 🦁\n",
      "Input Device id  4  -  Microsoft Teams Audio\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream Started !\n",
      "recording via index 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 11:14:52.346588: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-18 11:14:52.416627: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-18 11:14:52.716875: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.183295e-05]]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 11:14:53.554133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-18 11:14:53.697794: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-18 11:14:53.990535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-18 11:14:54.268797: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-18 11:14:54.550208: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.69808e-05]]\n",
      "0\n",
      "[[4.4716682e-05]]\n",
      "0\n",
      "[[3.9476417e-05]]\n",
      "0\n",
      "[[4.736921e-05]]\n",
      "0\n",
      "[[0.99999225]]\n",
      "1\n",
      "[[3.9296283e-05]]\n",
      "0\n",
      "[[4.1087307e-05]]\n",
      "0\n",
      "[[0.99996686]]\n",
      "1\n",
      "[[0.00047109]]\n",
      "0\n",
      "[[4.5356894e-05]]\n",
      "0\n",
      "Main Thread Terminated !\n",
      "Stream Stoped !\n",
      "[3.55800872e-02 8.87388882e-06 5.61238992e-05 1.81953707e-01\n",
      " 2.76417402e-01 9.08869346e-05 5.05892912e-01]\n"
     ]
    }
   ],
   "source": [
    "ed = emotion_detector('Models/SER.hdf5', \"Models/Gender_Classifier.hdf5\")\n",
    "device_index = ed.list_devices()\n",
    "ed.start_stream(device_index=device_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e137a88f-b0bc-42bd-86b3-0986e723122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAVDESS = \"Data/RAVDESS/audio_speech_actors_01-24/\"\n",
    "TESS = \"Data/TESS/\"\n",
    "datafiles = []\n",
    "for i in os.listdir(TESS):\n",
    "    datafiles.append(TESS + i)\n",
    "\n",
    "for i in os.listdir(RAVDESS):\n",
    "    if os.path.isdir(RAVDESS + i):\n",
    "        for j in os.listdir(RAVDESS + i):\n",
    "            datafiles.append(RAVDESS + i + '/' + j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140c4be-392f-4e85-b77c-e587c187130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = random.choice(datafiles)\n",
    "print(file)\n",
    "ed.start_stream(file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b181b-7d6b-4813-8f33-28c68cf7b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fef544-f659-453f-91c8-e3ff443c47ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0af627a34863016ed57d4b3106c0300251ee54f9e7c94bf74aecdb95dfc7bffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
